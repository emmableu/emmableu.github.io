[
  {
  "key": "wang2020crescendo",
  "month": 3,
  "date": null,
    "highlight": "A self-paced learning tool that allows novice students to do visual and interactive programming, while learning fundamental programming concepts.",
    "talk": "https://www.youtube.com/watch?v=OO10yjXG07w&t=1s",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vQI4q3m6oc1sQgVXlHmGf00RIgM-pkuKqXQ7taHf7Y_kQEP_jYv0BFD_10vMdkS765CeLC3l8PdX9Cg/pub?start=false&loop=false&delayms=60000",
    "rate": "31.4% acceptance rate; 171/544 papers.",
    "conferenceKey": "SIGCSE'20",
    "conferenceWithoutYear": "SIGCSE",
    "paperType": "full conference",
    "abstractKey": "This paper introduces Crescendo, a self-paced programming practice environment that combines the block-based and visual, interactive programming of Snap, with the structured practices commonly found in Drill-and-Practice Environments. Crescendo supports students with Parsons problems to reduce problem complexity, Use-Modify-Create task progressions to gradually introduce new programming concepts, and automated feedback and assessment to support learning. In this work, we report on our experience deploying Crescendo in a programming camp for middle school students, as well as in an introductory university course for non-majors. Our initial results from field observations and log data suggest that the support features in Crescendo kept students engaged and allowed them to progress through programming concepts quickly. However, some students still struggled even with these highly-structured problems, requiring additional assistance, suggesting that even strong scaffolding may be insufficient to allow students to progress independently through the tasks.",
    "authorship": "lead-authored"
  },
  {
    "key": "wang2020step",
    "month": 6,
    "date": null,
    "highlight": "One challenge with programming Worked Examples is that they are offered prior to problem-solving, and may not help students when they are stuck in the middle of problem-solving. Step Tutor's design borrows insights from programming next-step hints, but teaches one meaningful step at a time.",
    "talk": "https://youtu.be/4uWfUg41sp4",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vSd5_bougY0gdeMiEkxMnkwV6y88LOVc0Kai41WVAGeJ1WofMK2ZZVaReFfbDyiKnT62O51UjZYBwVE/pub?start=false&loop=false&delayms=60000",
    "rate": "27.6% acceptance rate; 72/261 full papers.",
    "conferenceKey": "ITiCSE'20",
    "conferenceWithoutYear": "ITiCSE",
    "paperType": "full conference",
    "abstractKey": "Students often get stuck when programming independently, and need help to progress. Existing, automated feedback can help students progress, but it is unclear whether it ultimately leads to learning. We present Step Tutor, which helps struggling students during programming by presenting them with relevant, step-by-step examples. The goal of Step Tutor is to help students progress, and engage them in comparison, reflection, and learning. When a student requests help, Step Tutor adaptively selects an example to demonstrate the next meaningful step in the solution. It engages the student in comparing ``before'' and ``after'' code snapshots, and their corresponding visual output, and guides them to reflect on the changes. Step Tutor is a novel form of help that combines effective aspects of existing support features, such as hints and Worked Examples, to help students both progress and learn. To understand how students use Step Tutor, we asked nine undergraduate students to complete two programming tasks, with its help, and interviewed them about their experience. We present our qualitative analysis of students' experience, which shows us why and how they seek help from Step Tutor, and Step Tutor's affordances. These initial results suggest that students perceived that Step Tutor accomplished its goals of helping them to progress and learn.",
    "authorship": "lead-authored"
  },
  {
    "key": "wang2020comparing",
    "month": 7,
    "date": null,
    "highlight": "Syntax-based static code analysis start by representing code as vectors and feeding it into a machine-learning model. What feature engineering approach is ideal when transforming code into vectors? A random sample of 413 highly different Scratch projects is insufficient to tell.",
    "talk": "https://youtu.be/f3F3vz5-9Qc",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vRW0XuFroTG-zZM6vPjk5bHHFo3aWBIcAwCv4DCmUS-3CVINYNZ9FuMZhFavlhWPvNF-DhX2-z1o79e/pub?start=false&loop=false&delayms=60000",
    "conferenceKey": "CSEDM Workshop @ EDM'20",
    "conferenceWithoutYear": "CSEDM",
    "paperType": "workshop",
    "abstractKey": "Using machine learning to classify student code has many applications in computer science education, such as auto-grading, identifying struggling students from their code, and propagating feedback to address particular misconceptions. However, a fundamental challenge of using machine learning for code classification is how to represent program code as a vector to be processed by modern learning algorithms. A piece of programming code is structurally represented by an abstract syntax tree (AST), and a variety of approaches have been proposed to extract features from these ASTs to use in learning algorithms, but no work has directly compared their effectiveness. In this paper, we do so by comparing three different feature engineering approaches for classifying the behavior of novices' open-ended programming projects according to expert labels. In order to evaluate the effectiveness of these feature engineering approaches, we hand-labeled a dataset of novice programs from the Scratch repository to indicate the presence of five complex, game-related programming behaviors. We compared these feature engineering approaches by evaluating their classification effectiveness. Our results show that the three approaches perform similarly across different target labels. However, we also find evidence that all approaches led to overfitting, suggesting the need for future research to select and reduce code features, which may reveal advantages in more complex feature engineering approaches.",
    "authorship": "lead-authored"
  },
  {
    "key": "milliken2021planit",
    "month": 3,
    "date": null,
    "pdf": "https://dl.acm.org/doi/10.1145/3408877.3432552",
    "conferenceKey": "SIGCSE'21",
    "conferenceWithoutYear": "SIGCSE",
    "paperType": "full conference",
    "abstractKey": "Project-based learning can encourage and motivate students to learn through exploring their own interests, but introduces special challenges for novice programmers. Recent research has shown that novice students perceive themselves to be bad at programming, especially when they do not know how to start writing a program, or need to create a plan before getting started. In this paper, we present PlanIT, a guided planning tool integrated with the Snap! programming environment designed to help novices plan and program their open-ended projects. Within PlanIT, students can add a description for their project, use a to do list to help break down the steps of implementation, plan important elements of their program including actors, variables, and events, and view related example projects. We report findings from a pilot study of high school students using PlanIT, showing that students who used the tool learned to make more specific and actionable plans. Results from student interviews show they appreciate the guidance that PlanIT provides, as well as the affordances it offers to more quickly create program elements.",
    "authorship": "co-authored"
  },
  {
    "key": "shi2021toward",
    "month": 4,
    "date": null,
    "pdf": "https://arxiv.org/pdf/2103.04448.pdf",
    "conferenceKey": "LAK'21",
    "conferenceWithoutYear": "LAK",
    "paperType": "short conference",
    "abstractKey": "Understanding students' misconceptions is important for effective teaching and assessment. However, discovering such misconceptions manually can be time-consuming and laborious. Automated misconception discovery can address these challenges by highlighting patterns in student data, which domain experts can then inspect to identify misconceptions. In this work, we present a novel method for the semi-automated discovery of problem-specific misconceptions from students' program code in computing courses, using a state-of-the-art code classification model. We trained the model on a block-based programming dataset and used the learned embedding to cluster incorrect student submissions. We found these clusters correspond to specific misconceptions about the problem and would not have been easily discovered with existing approaches. We also discuss potential applications of our approach and how these misconceptions inform domain-specific insights into students' learning processes.",
    "authorship": "co-authored"
  },
  {
    "key": "wang2021novices",
    "month": 6,
    "date": 17,
    "rate": "31% acceptance rate; 84/275 full papers.",
    "conferenceKey": "ITiCSE'21",
    "conferenceWithoutYear": "ITiCSE",
    "paperType": "full conference",
    "abstractKey": "Open-ended programming increases students' motivation by allowing them to solve authentic problems and connect programming to their own interests. \nHowever, such open-ended projects are also challenging, as they often encourage students to explore new programming features and attempt tasks that they have not learned before. Code examples are effective learning materials for students and are well-suited to supporting open-ended programming. However, there is little work to understand how novices learn with examples during open-ended programming, and few real-world deployments of such tools. In this paper, we explore novices' learning barriers when interacting with code examples during open-ended programming. We deployed Example Helper, a tool that offers galleries of code examples to search and use, with 44 novice students in an introductory programming classroom, working on an open-ended project in Snap. We found three high-level barriers that novices encountered when using examples: decision, search and integration barriers. We discuss how these barriers arise and  design opportunities to address them.",
    "authorship": "lead-authored",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vS_MSKM70XdArBxirf7o900fGZEu69bwBSOBFqDuXPI6AHPDNwyFUxF9nMPjlBl5jrQNxHloMDgEunx/pub?start=false&loop=false&delayms=3000",
    "talk": "https://youtu.be/tCNVUwqx2Ec"
  },
  {
    "key": "wang2021snapcheck",
    "month": 6,
    "date": 18,
    "rate": "31% acceptance rate; 84/275 full papers.",
    "conferenceKey": "ITiCSE'21",
    "conferenceWithoutYear": "ITiCSE",
    "paperType": "full conference",
    "abstractKey": "Programming environments such as Snap, Scratch, and Processing engage learners by allowing them to create programming artifacts such as apps and games, with visual and interactive output. Learning programming with such a media-focused context has been shown to increase retention and success rate. However, assessing these visual, interactive projects requires time and laborious manual effort, and it is therefore difficult to offer automated or real-time feedback to students as they work. In this paper, we introduce SnapCheck, a dynamic testing framework for Snap that enables instructors to author test cases with Condition-Action templates. The goal of SnapCheck is to allow instructors or researchers to author property-based test cases that can automatically assess students' interactive programs with high accuracy. Our evaluation of SnapCheck on 162 code snapshots from a Pong game assignment in an introductory programming course shows that our automated testing framework achieves at least 98% accuracy over all rubric items, showing potentials to use SnapCheck for auto-grading and providing formative feedback to students.",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vR5qGAVKMrnXZkT5B_Bx28Acv5i5TbrScWRtLla1xF-irVKtyRIXEW3yy_i7vL-Diz1f1ZNJNq8kr9a/pub?start=false&loop=false&delayms=3000",
    "talk": "https://youtu.be/tqPNVOK1Wws",
    "authorship": "lead-authored"
  },
  {
    "key": "wang2021execution",
    "month": 6,
    "date": 29,
    "conferenceKey": "CSEDM Workshop @ EDM'21",
    "conferenceWithoutYear": "CSEDM",
    "paperType": "workshop",
    "abstractKey": "Offering students immediate, formative feedback when they are programming can increase students' learning outcomes and self-efficacy. However, visual and interactive programs include dynamic user input and visual outputs that change over time, making it difficult to automatically assess students' code with traditional functional tests to offer this feedback. In this work, we introduce Execution Trace Based Feature Engineering (ETF), a feature engineering approach that extracts sequential patterns from execution traces, which capture the runtime behavior of students' code. We evaluated ETF on 162 students' code snapshots from a Pong game assignment in an introductory programming course, on a challenging task to predict students' success on fine-grained rubrics. We found that ETF achieves an average F1 score of 0.93 over 10 grading rubrics, which is 0.1-0.2 higher than a high-performing syntax-based code classification approach from prior work. These results show that ETF has strong potential to be used for code classification, to enable formative feedback for students' visual, interactive programs. ",
    "authorship": "lead-authored",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vQp1WwfRaW9Ufx39GKvntqlhbNNZafkAEagSv35SuT8d_fqvpA5o_Q6XPXJCIVhZfcTN2-2LQWEMj0c/pub?start=false&loop=false&delayms=3000",
    "talk": "https://youtu.be/EFlPQJMkA70"
  },
  {
    "key": "wang2022exploring",
    "month": 3,
    "date": null,
    "conferenceKey": "SIGCSE'22",
    "conferenceWithoutYear": "SIGCSE",
    "abstractKey": "Open-ended programming engages students by connecting computing with their real-world experience and personal interest. However, such open-ended programming tasks can be challenging, as they require students to implement features that they may be unfamiliar with. Code examples help students to generate ideas and implement program features, but students also encounter many learning barriers when using them. We explore how to design code examples to support novices' effective example use by presenting our experience of building and deploying Example Helper, a system that supports students with a gallery of code examples during open-ended programming. We deployed Example Helper in an undergraduate  CS0 classroom to investigate students' example usage experience, finding that students used different strategies to browse, understand, experiment with, and integrate code examples, and that students who make more sophisticated plans also used more examples in their projects.",
    "paperType": "full conference",
    "authorship": "lead-authored",
    "slides": "https://docs.google.com/presentation/d/e/2PACX-1vTXX1BoqrYvJ3RpzAv6K6JT33AA5UOkoYJtVUNjSl6778I7oIkywmf2RcOIK_6JSDOfH27A4ydiE6lv/pub?start=false&loop=false&delayms=3000",
    "talk": "https://youtu.be/nW18PJpY2rs"
  },
  {
    "key": "wang2022pinpoint",
    "month": 8,
    "date": null,
    "conferenceKey": "VL/HCC'22",
    "conferenceWithoutYear": "VL/HCC",
    "abstractKey": "Block-based programming environments, such as Scratch and Snap!, engage users to create programming artifacts such as games and stories, and share them in an online community. Many Snap! users start programming by reusing and modifying an example project, but encounter many barriers when searching and identifying the relevant parts of the program to learn and reuse. We present Pinpoint, a system that helps Snap! programmers understand and reuse an existing program by isolating the code responsible for specific events during program execution. Specifically, a user can record an execution of the program (including user inputs and graphical output), replay the output, and select a specific time interval where the event of interest occurred, to view code that is relevant to this event. We conducted a small-scale user study to compare users&amp;#x2019; program comprehension experience with and without Pinpoint, and found suggestive evidence that Pinpoint helps users understand and reuse a complex program more efficiently.",
    "paperType": "full conference",
    "authorship": "lead-authored"
  },
  {
    "key": "deiner2022automated",
    "month": 5,
    "date": null,
    "conferenceKey": "Empirical Software Engineering",
    "conferenceWithoutYear": "ESE",
    "abstractKey": "The importance of programming education has lead to dedicated educational programming environments, where users visually arrange block-based programming constructs that typically control graphical, interactive game-like programs. The Scratch programming environment is particularly popular, with more than 70 million registered users at the time of this writing. While the block-based nature of Scratch helps learners by preventing syntactical mistakes, there nevertheless remains a need to provide feedback and support in order to implement desired functionality. To support individual learning and classroom settings, this feedback and support should ideally be provided in an automated fashion, which requires tests to enable dynamic program analysis. The Whisker framework enables automated testing of Scratch programs, but creating these automated tests for Scratch programs is challenging. In this paper, we therefore investigate how to automatically generate Whisker tests. This raises important challenges: First, game-like programs are typically randomised, leading to flaky tests. Second, Scratch programs usually consist of animations and interactions with long delays, inhibiting the application of classical test generation approaches. Evaluation on common programming exercises, a random sample of 1000 Scratch user programs, and the 1000 most popular Scratch programs demonstrates that our approach enables Whisker to reliably accelerate test executions, and even though many Scratch programs are small and easy to cover, there are many unique challenges for which advanced search-based test generation using many-objective algorithms is needed in order to achieve high coverage. ",
    "paperType": "journal",
    "authorship": "lead-authored"
  },
  {
    "key": "wang2023design",
    "month": 1,
    "date": null,
    "conferenceKey": "Thesis",
    "conferenceWithoutYear": "Thesis",
    "abstractKey": "Open-ended programming engages students by connecting computing with their realworld experience and personal interests. However, such open-ended programming tasks can be challenging, as they require students to implement features that may require knowledge students are unfamiliar with. Code examples can help students to generate ideas and implement those features, but students often face barriers when searching, learning, and integrating those examples. This work explores what challenges students face when using code examples during open-ended programming, and how to automatically generate personalized code examples, and how to design systems to present examples, in order to improve students’ programming performance, sense of ownership, and perceived creativity. In this work, I investigate three research questions: 1) What are novices’ motivations, strategies, and barriers when using code examples during open-ended programming? 2) How can we design code examples to address students’ decision, search, mapping, understanding, and testing barriers? 3) What is the impact of having access to code examples on students’ open-ended programming? I present 5 studies to address these research questions.",
    "paperType": "thesis",
    "authorship": "lead-authored"
  },
  {
    "key": "wang2023case",
    "month": 3,
    "date": null,
    "conferenceKey": "ITiCSE'23 (preprint)",
    "conferenceWithoutYear": "ITiCSE",
    "abstractKey": "Many students rely on examples when learning to program, but they often face barriers to incorporating these examples into their own code and learning the concepts they present. As a step towards designing effective example interfaces that can support student learning, we investigate novices' needs and strategies when using examples to write code. We conducted a study with 12 pairs of high school students working on open-ended game design projects, using a system that allows students to browse examples based on their output, and to view and copy the example code. We analyzed interviews, screen recordings, and log data, identifying 5 moments when novices request examples, and 4 strategies that arise when students use examples. We synthesize these findings into principles that can inform the design of future example systems to better support students.",
    "paperType": "full conference",
    "authorship": "lead-authored"
  }
]
